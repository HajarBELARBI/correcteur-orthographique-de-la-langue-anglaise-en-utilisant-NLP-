{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce68d75",
   "metadata": {
    "id": "5ce68d75"
   },
   "source": [
    "# Algo 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b1d54e",
   "metadata": {
    "id": "61b1d54e"
   },
   "source": [
    "la distance minimale d'edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ce00f6",
   "metadata": {
    "executionInfo": {
     "elapsed": 461,
     "status": "ok",
     "timestamp": 1694973684584,
     "user": {
      "displayName": "ces moi",
      "userId": "16355593303724342328"
     },
     "user_tz": -120
    },
    "id": "c7ce00f6"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a25fb3b",
   "metadata": {
    "id": "3a25fb3b"
   },
   "source": [
    " \"process_data\", prend en paramètre un nom de fichier. Elle lit le fichier, extrait les mots du contenu du fichier, les met en minuscules et les retourne sous forme d'une liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c6bbf",
   "metadata": {
    "id": "129c6bbf"
   },
   "outputs": [],
   "source": [
    "def process_data(file_name):\n",
    "\n",
    "    words = []\n",
    "    with open(file_name,encoding=\"utf8\") as f:\n",
    "        file_name_data = f.read()\n",
    "    file_name_data=file_name_data.lower()\n",
    "    words = re.findall('\\w+',file_name_data)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235eed5b",
   "metadata": {
    "id": "235eed5b"
   },
   "source": [
    "\"get_count\", prend en paramètre un ensemble de mots représentant le corpus (une collection de mots). Elle compte la fréquence de chaque mot dans le corpus et renvoie un dictionnaire où la clé est le mot et la valeur est sa fréquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf4e41",
   "metadata": {
    "id": "9bcf4e41"
   },
   "outputs": [],
   "source": [
    "def get_count(word_l):\n",
    "\n",
    "    word_count_dict = {}\n",
    "    word_count_dict = Counter(word_l)\n",
    "\n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10742e3c",
   "metadata": {
    "id": "10742e3c"
   },
   "source": [
    "\"get_probs\", prend en paramètre un dictionnaire de décompte de mots où la clé est le mot et la valeur est sa fréquence. Elle calcule les probabilités que chaque mot se produise dans le corpus et renvoie un dictionnaire où les clés sont les mots et les valeurs sont les probabilités correspondantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa814c35",
   "metadata": {
    "id": "fa814c35"
   },
   "outputs": [],
   "source": [
    "def get_probs(word_count_dict):\n",
    "\n",
    "    probs = {}\n",
    "\n",
    "    m = sum(word_count_dict.values())\n",
    "    for key in word_count_dict.keys():\n",
    "        probs[key] = word_count_dict[key] / m\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ac4b6",
   "metadata": {
    "id": "5d0ac4b6"
   },
   "source": [
    "delete_letter\", prend en paramètre une chaîne de caractères (mot) et génère toutes les chaînes de caractères possibles dans le vocabulaire qui ont un caractère manquant par rapport au mot d'entrée. Elle retourne une liste contenant toutes ces chaînes de caractères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fce788f",
   "metadata": {
    "id": "3fce788f"
   },
   "outputs": [],
   "source": [
    "def delete_letter(word, verbose=False):\n",
    "\n",
    "    delete_l = []\n",
    "    split_l = []\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    for c in range(len(word)):\n",
    "        split_l.append((word[:c],word[c:]))\n",
    "    for a,b in split_l:\n",
    "        delete_l.append(a+b[1:])\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    if verbose: print(f\"input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n",
    "\n",
    "    return delete_l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e180e3d5",
   "metadata": {
    "id": "e180e3d5"
   },
   "source": [
    "\"replace_letter\", prend en paramètre une chaîne de caractères (mot) et génère toutes les chaînes de caractères possibles où un seul caractère du mot original a été remplacé par une lettre différente de l'alphabet. Elle retourne une liste contenant toutes ces chaînes de caractères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a607d",
   "metadata": {
    "id": "a03a607d"
   },
   "outputs": [],
   "source": [
    "def replace_letter(word, verbose=False):\n",
    "\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    replace_l = []\n",
    "    split_l = []\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    for c in range(len(word)):\n",
    "        split_l.append((word[0:c],word[c:]))\n",
    "    replace_l = [a + l + (b[1:] if len(b)> 1 else '') for a,b in split_l if b for l in letters]\n",
    "    replace_set=set(replace_l)\n",
    "    replace_set.remove(word)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # turn the set back into a list and sort it, for easier viewing\n",
    "    replace_l = sorted(list(replace_set))\n",
    "\n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")\n",
    "\n",
    "    return replace_l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2417d9da",
   "metadata": {
    "id": "2417d9da"
   },
   "source": [
    "\"switch_letter\", prend en paramètre une chaîne de caractères (mot) et génère toutes les chaînes de caractères possibles où deux caractères adjacents du mot ont été échangés de position. Elle retourne une liste contenant toutes ces chaînes de caractères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0572ecd0",
   "metadata": {
    "id": "0572ecd0"
   },
   "outputs": [],
   "source": [
    "def switch_letter(word, verbose=False):\n",
    "\n",
    "    switch_l = []\n",
    "    split_l = []\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    len_word=len(word)\n",
    "    for c in range(len_word):\n",
    "        split_l.append((word[:c],word[c:]))\n",
    "    switch_l = [a + b[1] + b[0] + b[2:] for a,b in split_l if len(b) >= 2]\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\")\n",
    "\n",
    "    return switch_l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d0ec8c",
   "metadata": {
    "id": "52d0ec8c"
   },
   "source": [
    "\"insert_letter\", prend en paramètre une chaîne de caractères (mot) et génère toutes les chaînes de caractères possibles où une nouvelle lettre est insérée à chaque position possible du mot d'origine. Elle retourne un ensemble (pour éviter les duplications) contenant toutes ces chaînes de caractères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5984f0",
   "metadata": {
    "id": "3a5984f0"
   },
   "outputs": [],
   "source": [
    "def insert_letter(word, verbose=False):\n",
    "\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    insert_l = []\n",
    "    split_l = []\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    for c in range(len(word)+1):\n",
    "        split_l.append((word[0:c],word[c:]))\n",
    "    insert_l = [ a + l + b for a,b in split_l for l in letters]\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n",
    "\n",
    "    return insert_l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36c6952",
   "metadata": {
    "id": "a36c6952"
   },
   "source": [
    "\"edit_one_letter\", prend en paramètre une chaîne de caractères (mot) et génère un ensemble (set) de toutes les chaînes de caractères possibles qui sont à une seule modification près du mot d'origine. Une modification peut être une suppression d'un caractère, un échange de positions de deux caractères adjacents, le remplacement d'un caractère par un autre, ou l'insertion d'une nouvelle lettre à une position donnée. L'argument optionnel \"allow_switches\" indique si les échanges de positions de caractères adjacents sont autorisés ou non."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54487c9f",
   "metadata": {
    "id": "54487c9f"
   },
   "outputs": [],
   "source": [
    "def edit_one_letter(word, allow_switches = True):\n",
    "\n",
    "    edit_one_set = set()\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    edit_one_set.update(delete_letter(word))\n",
    "    if allow_switches:\n",
    "        edit_one_set.update(switch_letter(word))\n",
    "    edit_one_set.update(replace_letter(word))\n",
    "    edit_one_set.update(insert_letter(word))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return edit_one_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec43bce",
   "metadata": {
    "id": "4ec43bce"
   },
   "source": [
    "\"edit_two_letters\", prend en paramètre une chaîne de caractères (mot) et génère un ensemble (set) de toutes les chaînes de caractères possibles qui sont à deux modifications près du mot d'origine. Les modifications peuvent être une suppression, un échange, un remplacement ou une insertion d'un ou plusieurs caractères. L'argument optionnel \"allow_switches\" indique si les échanges de positions de caractères adjacents sont autorisés ou non."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7291837e",
   "metadata": {
    "id": "7291837e"
   },
   "outputs": [],
   "source": [
    "def edit_two_letters(word, allow_switches = True):\n",
    "\n",
    "    edit_two_set = set()\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    edit_one = edit_one_letter(word,allow_switches=allow_switches)\n",
    "    for w in edit_one:\n",
    "        if w:\n",
    "            edit_two = edit_one_letter(w,allow_switches=allow_switches)\n",
    "            edit_two_set.update(edit_two)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return edit_two_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80d9c39",
   "metadata": {
    "id": "e80d9c39"
   },
   "source": [
    "\"edit_n_letters\", prend en paramètre une chaîne de caractères (mot), le nombre d'éditions souhaité \"n\" et un argument optionnel \"allow_switches\" qui indique si les échanges de positions de caractères adjacents sont autorisés ou non. Elle génère un ensemble (set) de toutes les chaînes de caractères possibles qui sont à \"n\" modifications près du mot d'origine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afb3a49",
   "metadata": {
    "id": "1afb3a49"
   },
   "outputs": [],
   "source": [
    "def edit_n_letters(word, n, allow_switches=True):\n",
    "\n",
    "    if n == 0:\n",
    "        return {word}\n",
    "\n",
    "    edit_n_set = set()\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    edit_one = edit_one_letter(word, allow_switches=allow_switches)\n",
    "    for w in edit_one:\n",
    "        if w:\n",
    "            edit_n = edit_n_letters(w, n-1, allow_switches=allow_switches)\n",
    "            edit_n_set.update(edit_n)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return edit_n_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df71cc66",
   "metadata": {
    "id": "df71cc66"
   },
   "source": [
    "\"get_corrections\", prend en paramètre un mot saisi par l'utilisateur (\"word\"), un dictionnaire de probabilités associant chaque mot à sa probabilité dans le corpus (\"probs\"), un ensemble contenant tout le vocabulaire (\"vocab\"), le nombre de corrections souhaitées (\"n\"), et un argument optionnel pour afficher les informations détaillées (\"verbose\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f428acf",
   "metadata": {
    "id": "2f428acf"
   },
   "outputs": [],
   "source": [
    "def get_corrections(word, probs, vocab, n=2, verbose = False):\n",
    "\n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    suggestions = list((word in vocab and word) or edit_one_letter(word).intersection(vocab) or edit_two_letters(word).intersection(vocab))\n",
    "    n_best = [[s,probs[s]] for s in list(reversed(suggestions))]\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    if verbose: print(\"suggestions = \", suggestions)\n",
    "\n",
    "    return n_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346404e6",
   "metadata": {
    "id": "346404e6"
   },
   "source": [
    "- pour lire le contenu du fichier texte et obtenir une liste de mots en minuscules. Cette liste est stockée dans la variable word_l.\n",
    "- Il crée un ensemble vocab à partir de la liste de mots, ce qui représente l'ensemble de tous les mots du vocabulaire.\n",
    "- appelle la fonction get_count(word_l) pour obtenir un dictionnaire de comptage des mots dans la liste word_l. Ce dictionnaire est ensuite utilisé comme argument pour la fonction get_probs() afin d'obtenir un dictionnaire de probabilités probs, où chaque mot est associé à sa probabilité.\n",
    "- appelle la fonction get_corrections(\"cler\", probs, vocab) pour obtenir une liste des meilleures corrections possibles du mot \"cler\" en utilisant les probabilités et le vocabulaire disponibles. Cette liste est stockée dans la variable n_best.\n",
    "- vérifie si la liste n_best n'est pas vide en utilisant la condition if n_best. Si elle n'est pas vide, cela signifie qu'il y a des corrections possibles.\n",
    "- utilise la fonction max() pour trouver la meilleure correction dans la liste n_best en se basant sur la probabilité associée à chaque correction. La fonction key=lambda x: x[1] est utilisée pour spécifier que la comparaison doit se faire sur la deuxième valeur de chaque tuple (la probabilité).\n",
    "- affiche le mot corrigé en accédant au premier élément du tuple best_word (le mot corrigé) et l'affiche avec la phrase \"Corrected word:\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7610c8f6",
   "metadata": {
    "id": "7610c8f6",
    "outputId": "8c25f6a8-cc24-45d4-ead3-7238d98cc8e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected word: speaking\n"
     ]
    }
   ],
   "source": [
    "word_l = process_data('input.txt')\n",
    "vocab = set(word_l)\n",
    "probs = get_probs(get_count(word_l))\n",
    "n_best = get_corrections(\"spealing\", probs, vocab)\n",
    "if n_best:\n",
    "    best_word = max(n_best, key=lambda x: x[1])\n",
    "    print(\"Corrected word:\", best_word[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423895ea",
   "metadata": {
    "id": "423895ea"
   },
   "source": [
    "# Algo 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94fe338",
   "metadata": {
    "id": "a94fe338"
   },
   "source": [
    " un modele de langue pour ordonner les corrections possibles par leurs probabilit´es\n",
    "selon le modele de langue et selon la distance minimale d’´edition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c812ec3",
   "metadata": {
    "id": "9c812ec3"
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764554a6",
   "metadata": {
    "id": "764554a6"
   },
   "outputs": [],
   "source": [
    "with open(\"input.txt\", \"r\",encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506305e7",
   "metadata": {
    "id": "506305e7"
   },
   "source": [
    "Prend en entrée une chaîne de caractères (data) et retourne une liste de phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f3178",
   "metadata": {
    "id": "998f3178"
   },
   "outputs": [],
   "source": [
    "def split_to_sentences(data):\n",
    "    \"\"\"\n",
    "    Split data by linebreak \"\\n\"\n",
    "\n",
    "    Args:\n",
    "        data: str\n",
    "\n",
    "    Returns:\n",
    "        A list of sentences\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    sentences = data.split('\\n')\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Additional clearning (This part is already implemented)\n",
    "    # - Remove leading and trailing spaces from each sentence\n",
    "    # - Drop sentences if they are empty strings.\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a39fd4e",
   "metadata": {
    "id": "5a39fd4e"
   },
   "source": [
    "prend en entrée une liste de phrases (sentences) et retourne une liste de listes de jetons (mots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f0d22a",
   "metadata": {
    "id": "44f0d22a"
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Tokenize sentences into tokens (words)\n",
    "\n",
    "    Args:\n",
    "        sentences: List of strings\n",
    "\n",
    "    Returns:\n",
    "        List of lists of tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the list of lists of tokenized sentences\n",
    "    tokenized_sentences = []\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "    # Go through each sentence\n",
    "    for sentence in sentences:\n",
    "\n",
    "        # Convert to lowercase letters\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "        # Convert into a list of words\n",
    "        tokenized = nltk.word_tokenize(sentence)\n",
    "\n",
    "        # append the list of words to the list of lists\n",
    "        tokenized_sentences.append(tokenized)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086b871",
   "metadata": {
    "id": "4086b871"
   },
   "source": [
    "prend en entrée une chaîne de caractères (data) et retourne une liste de listes de jetons (mots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b623df2d",
   "metadata": {
    "id": "b623df2d"
   },
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "### GRADED_FUNCTION: get_tokenized_data ###\n",
    "def get_tokenized_data(data):\n",
    "    \"\"\"\n",
    "    Make a list of tokenized sentences\n",
    "\n",
    "    Args:\n",
    "        data: String\n",
    "\n",
    "    Returns:\n",
    "        List of lists of tokens\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "    # Get the sentences by splitting up the data\n",
    "    sentences = split_to_sentences(data)\n",
    "\n",
    "    # Get the list of lists of tokens by tokenizing the sentences\n",
    "    tokenized_sentences = tokenize_sentences(sentences)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13b138",
   "metadata": {
    "id": "1f13b138"
   },
   "source": [
    "générer et mélanger aléatoirement les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d576f",
   "metadata": {
    "id": "018d576f"
   },
   "outputs": [],
   "source": [
    "data = get_tokenized_data(data)\n",
    "random.seed(87)\n",
    "random.shuffle(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfbfa56",
   "metadata": {
    "id": "2dfbfa56"
   },
   "source": [
    "prend en entrée une liste de listes de chaînes de caractères (tokenized_sentences) et retourne un dictionnaire qui associe chaque mot (chaîne de caractères) à sa fréquence (entier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79401593",
   "metadata": {
    "id": "79401593"
   },
   "outputs": [],
   "source": [
    "def count_words(tokenized_sentences):\n",
    "    \"\"\"\n",
    "    Count the number of word appearence in the tokenized sentences\n",
    "\n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of strings\n",
    "\n",
    "    Returns:\n",
    "        dict that maps word (str) to the frequency (int)\n",
    "    \"\"\"\n",
    "\n",
    "    word_counts = {}\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "    # Loop through each sentence\n",
    "    for sentence in tokenized_sentences: # complete this line\n",
    "\n",
    "        # Go through each token in the sentence\n",
    "        for token in sentence: # complete this line\n",
    "\n",
    "            # If the token is not in the dictionary yet, set the count to 1\n",
    "            if token not in word_counts.keys(): # complete this line\n",
    "                word_counts[token] = 1\n",
    "\n",
    "            # If the token is already in the dictionary, increment the count by 1\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce8bf7",
   "metadata": {
    "id": "f3ce8bf7"
   },
   "source": [
    "prend en entrée une liste de listes de chaînes de caractères (tokenized_sentences) et un seuil de fréquence (count_threshold). Elle retourne une liste de mots qui apparaissent N fois ou plus, où N est égal ou supérieur au seuil de fréquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c748e9",
   "metadata": {
    "id": "33c748e9"
   },
   "outputs": [],
   "source": [
    "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
    "    \"\"\"\n",
    "    Find the words that appear N times or more\n",
    "\n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of sentences\n",
    "        count_threshold: minimum number of occurrences for a word to be in the closed vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        List of words that appear N times or more\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to contain the words that\n",
    "    # appear at least 'minimum_freq' times.\n",
    "    closed_vocab = []\n",
    "\n",
    "    # Get the word couts of the tokenized sentences\n",
    "    # Use the function that you defined earlier to count the words\n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "    # for each word and its count\n",
    "    for word, cnt in word_counts.items(): # complete this line\n",
    "\n",
    "        # check that the word's count\n",
    "        # is at least as great as the minimum count\n",
    "        if cnt >= count_threshold:\n",
    "\n",
    "            # append the word to the list\n",
    "            closed_vocab.append(word)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return closed_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33cee4a",
   "metadata": {
    "id": "a33cee4a"
   },
   "source": [
    "prend en entrée une liste de listes de chaînes de caractères (tokenized_sentences), un vocabulaire de référence (vocabulary), et un jeton représentant les mots inconnus ou hors vocabulaire (unknown_token par défaut \"<unk>\"). Elle retourne une liste de listes de chaînes de caractères où les mots qui ne se trouvent pas dans le vocabulaire de référence sont remplacés par le jeton <unk>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc333eb7",
   "metadata": {
    "id": "bc333eb7"
   },
   "outputs": [],
   "source": [
    "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "    \"\"\"\n",
    "    Replace words not in the given vocabulary with '<unk>' token.\n",
    "\n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of strings\n",
    "        vocabulary: List of strings that we will use\n",
    "        unknown_token: A string representing unknown (out-of-vocabulary) words\n",
    "\n",
    "    Returns:\n",
    "        List of lists of strings, with words not in the vocabulary replaced\n",
    "    \"\"\"\n",
    "\n",
    "    # Place vocabulary into a set for faster search\n",
    "    vocabulary = set(vocabulary)\n",
    "\n",
    "    # Initialize a list that will hold the sentences\n",
    "    # after less frequent words are replaced by the unknown token\n",
    "    replaced_tokenized_sentences = []\n",
    "\n",
    "    # Go through each sentence\n",
    "    for sentence in tokenized_sentences:\n",
    "\n",
    "        # Initialize the list that will contain\n",
    "        # a single sentence with \"unknown_token\" replacements\n",
    "        replaced_sentence = []\n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "        # for each token in the sentence\n",
    "        for token in sentence: # complete this line\n",
    "\n",
    "            # Check if the token is in the closed vocabulary\n",
    "            if token in vocabulary: # complete this line\n",
    "                # If so, append the word to the replaced_sentence\n",
    "                replaced_sentence.append(token)\n",
    "            else:\n",
    "                # otherwise, append the unknown token instead\n",
    "                replaced_sentence.append(unknown_token)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Append the list of tokens to the list of lists\n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "\n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4565d4dd",
   "metadata": {
    "id": "4565d4dd"
   },
   "source": [
    "prend en entrée une liste de listes de mots (data), le nombre d'éléments dans une séquence (n), ainsi que des jetons optionnels pour le début et la fin de la séquence (start_token et end_token respectivement). Elle retourne un dictionnaire qui associe chaque tuple de n mots à sa fréquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ab4fd",
   "metadata": {
    "id": "174ab4fd"
   },
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
    "    \"\"\"\n",
    "    Count all n-grams in the data\n",
    "\n",
    "    Args:\n",
    "        data: List of lists of words\n",
    "        n: number of words in a sequence\n",
    "\n",
    "    Returns:\n",
    "        A dictionary that maps a tuple of n-words to its frequency\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize dictionary of n-grams and their counts\n",
    "    n_grams = {}\n",
    "\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "    # Go through each sentence in the data\n",
    "    for sentence in data: # complete this line\n",
    "\n",
    "        # prepend start token n times, and  append <e> one time\n",
    "        sentence = [start_token] * n+ sentence + [end_token]\n",
    "\n",
    "        # convert list to tuple\n",
    "        # So that the sequence of words can be used as\n",
    "        # a key in the dictionary\n",
    "        sentence = tuple(sentence)\n",
    "\n",
    "        # Use 'i' to indicate the start of the n-gram\n",
    "        # from index 0\n",
    "        # to the last index where the end of the n-gram\n",
    "        # is within the sentence.\n",
    "        m = len(sentence) if n==1 else len(sentence)-1\n",
    "        for i in range(m): # complete this line\n",
    "\n",
    "            # Get the n-gram from i to i+n\n",
    "            n_gram = sentence[i:i+n]\n",
    "\n",
    "            # check if the n-gram is in the dictionary\n",
    "            if n_gram in n_grams.keys(): # complete this line\n",
    "\n",
    "                # Increment the count for this n-gram\n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                # Initialize this n-gram count to 1\n",
    "                n_grams[n_gram] = 1\n",
    "\n",
    "            ### END CODE HERE ###\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f101f7",
   "metadata": {
    "id": "34f101f7"
   },
   "source": [
    "estime les probabilités d'un prochain mot en utilisant les comptages des n-garm avec le lissage k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310df56d",
   "metadata": {
    "id": "310df56d"
   },
   "outputs": [],
   "source": [
    "# UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "### GRADED FUNCTION: estimate_probabilityy ###\n",
    "def estimate_probability(word, previous_n_gram,\n",
    "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
    "\n",
    "    Args:\n",
    "        word: next word\n",
    "        previous_n_gram: A sequence of words of length n\n",
    "        n_gram_counts: Dictionary of counts of n-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary_size: number of words in the vocabulary\n",
    "        k: positive constant, smoothing parameter\n",
    "\n",
    "    Returns:\n",
    "        A probability\n",
    "    \"\"\"\n",
    "    # convert list to tuple to use it as a dictionary key\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "    # Set the denominator\n",
    "    # If the previous n-gram exists in the dictionary of n-gram counts,\n",
    "    # Get its count.  Otherwise set the count to zero\n",
    "    # Use the dictionary that has counts for n-grams\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts  else 0\n",
    "\n",
    "    # Calculate the denominator using the count of the previous n gram\n",
    "    # and apply k-smoothing\n",
    "    denominator = previous_n_gram_count + k * vocabulary_size\n",
    "\n",
    "    # Define n plus 1 gram as the previous n-gram plus the current word as a tuple\n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    "\n",
    "    # Set the count to the count in the dictionary,\n",
    "    # otherwise 0 if not in the dictionary\n",
    "    # use the dictionary that has counts for the n-gram plus current word\n",
    "    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts  else 0\n",
    "\n",
    "    # Define the numerator use the count of the n-gram plus current word,\n",
    "    # and apply smoothing\n",
    "    numerator = n_plus1_gram_count + k\n",
    "\n",
    "    # Calculate the probability as the numerator divided by denominator\n",
    "    probability = numerator / denominator\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a2f57f",
   "metadata": {
    "id": "44a2f57f"
   },
   "source": [
    "estime les probabilités des prochains mots en utilisant les comptages des n-grammes avec le lissage k.\n",
    "itère sur chaque mot du vocabulaire et appelle la fonction estimate_probability pour estimer la probabilité de ce mot en fonction du n-gramme précédent et des comptages des n-grammes et des (n+1)-grammes. Les probabilités estimées sont stockées dans un dictionnaire et retournées à la fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fdd7dc",
   "metadata": {
    "id": "07fdd7dc"
   },
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of next words using the n-gram counts with k-smoothing\n",
    "\n",
    "    Args:\n",
    "        previous_n_gram: A sequence of words of length n\n",
    "        n_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary: List of words\n",
    "        k: positive constant, smoothing parameter\n",
    "\n",
    "    Returns:\n",
    "        A dictionary mapping from next words to the probability.\n",
    "    \"\"\"\n",
    "\n",
    "    # convert list to tuple to use it as a dictionary key\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "\n",
    "    # add <e> <unk> to the vocabulary\n",
    "    # <s> is not needed since it should not appear as the next word\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "\n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probability = estimate_probability(word, previous_n_gram,\n",
    "                                           n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary_size, k=k)\n",
    "        probabilities[word] = probability\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11468640",
   "metadata": {
    "id": "11468640"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, count_threshold):\n",
    "    \"\"\"\n",
    "    Preprocess data, i.e.,\n",
    "        - Find tokens that appear at least N times in the training data.\n",
    "        - Replace tokens that appear less than N times by \"<unk>\" both for training and test data.\n",
    "    Args:\n",
    "        train_data, test_data: List of lists of strings.\n",
    "        count_threshold: Words whose count is less than this are\n",
    "                      treated as unknown.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of\n",
    "        - training data with low frequent words replaced by \"<unk>\"\n",
    "        - test data with low frequent words replaced by \"<unk>\"\n",
    "        - vocabulary of words that appear n times or more in the training data\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "    # Get the closed vocabulary using the train data\n",
    "    vocabulary = get_words_with_nplus_frequency(train_data,count_threshold)\n",
    "\n",
    "    # For the train data, replace less common words with \"<unk>\"\n",
    "    train_data_replaced = replace_oov_words_by_unk(train_data,vocabulary)\n",
    "\n",
    "#     # For the test data, replace less common words with \"<unk>\"\n",
    "#     test_data_replaced = replace_oov_words_by_unk(test_data,vocabulary)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return train_data_replaced, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bfc1be",
   "metadata": {
    "id": "43bfc1be"
   },
   "outputs": [],
   "source": [
    "minimum_freq = 2\n",
    "data, vocabulary = preprocess_data(data, minimum_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20a3afd",
   "metadata": {
    "id": "d20a3afd"
   },
   "source": [
    "l'algorithme de distance d'édition (minimum edit distance) entre deux chaînes de caractères source et target. La distance d'édition mesure le nombre minimum d'opérations nécessaires pour transformer la chaîne source en la chaîne target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75bb79b",
   "metadata": {
    "id": "d75bb79b"
   },
   "outputs": [],
   "source": [
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
    "\n",
    "    # use deletion and insert cost as  1\n",
    "    m = len(source)\n",
    "    n = len(target)\n",
    "    #initialize cost matrix with zeros and dimensions (m+1,n+1)\n",
    "    D = np.zeros((m+1, n+1), dtype=int)\n",
    "\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "    # Fill in column 0, from row 1 to row m, both inclusive\n",
    "    for row in range(1,m+1): # Replace None with the proper range\n",
    "        D[row,0] = D[row-1,0] + del_cost\n",
    "\n",
    "    # Fill in row 0, for all columns from 1 to n, both inclusive\n",
    "    for col in range(1,n+1): # Replace None with the proper range\n",
    "        D[0,col] = D[0,col-1] + ins_cost\n",
    "\n",
    "    # Loop through row 1 to row m, both inclusive\n",
    "    for row in range(1,m+1):\n",
    "\n",
    "        # Loop through column 1 to column n, both inclusive\n",
    "        for col in range(1,n+1):\n",
    "\n",
    "            # Intialize r_cost to the 'replace' cost that is passed into this function\n",
    "            r_cost = rep_cost\n",
    "\n",
    "            # Check to see if source character at the previous row\n",
    "            # matches the target character at the previous column,\n",
    "            if source[row-1] == target[col-1]:\n",
    "                # Update the replacement cost to 0 if source and target are the same\n",
    "                r_cost = 0\n",
    "\n",
    "            # Update the cost at row, col based on previous entries in the cost matrix\n",
    "            # Refer to the equation calculate for D[i,j] (the minimum of three calculated costs)\n",
    "            D[row,col] = min([D[row-1,col]+del_cost, D[row,col-1]+ins_cost, D[row-1,col-1]+r_cost])\n",
    "\n",
    "    # Set the minimum edit distance with the cost found at row m, column n\n",
    "    med = D[m,n]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return D, med\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfdfc03",
   "metadata": {
    "id": "5bfdfc03"
   },
   "source": [
    "get_corrections prend en entrée un mot à vérifier et retourne une liste des n mots corrigés les plus probables, ainsi que leurs probabilités et distances d'édition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f30cab3",
   "metadata": {
    "id": "6f30cab3"
   },
   "outputs": [],
   "source": [
    "def get_corrections(word, n_gram_counts, n_plus1_gram_counts, vocabulary, vocab_size, n=2):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        word: a user-entered string to check for suggestions\n",
    "        n_gram_counts: Dictionary of counts of n-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary: List of words\n",
    "        vocab_size: Size of vocabulary\n",
    "        n: number of possible word corrections you want returned in the dictionary\n",
    "    Output:\n",
    "        n_best: a list of tuples with the most probable n corrected words, their probabilities, and edit distances.\n",
    "    \"\"\"\n",
    "\n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "\n",
    "    # Check if the word is already in the vocabulary\n",
    "    if word in vocabulary:\n",
    "        return [(word, 1.0, 0)]\n",
    "\n",
    "    # Estimate probabilities of next words using n-gram counts\n",
    "    previous_n_gram = word[:-1]\n",
    "    probabilities = estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary)\n",
    "\n",
    "    # Edit distance of 1\n",
    "    suggestions.extend(edit_one_letter(word))\n",
    "\n",
    "    # Edit distance of 2\n",
    "    suggestions.extend(edit_two_letters(word))\n",
    "\n",
    "    # Calculate edit distances and probabilities for suggestions\n",
    "    n_best = [(suggestion, probabilities.get(suggestion, 0.0), min_edit_distance(word, suggestion)[1]) for suggestion in suggestions]\n",
    "\n",
    "    # Sort the suggestions based on probabilities and edit distances\n",
    "    n_best = sorted(n_best, key=lambda x: (x[1], x[2]), reverse=True)\n",
    "\n",
    "    if len(n_best) >= n:\n",
    "        return n_best[:n]\n",
    "\n",
    "    return n_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f028a9ef",
   "metadata": {
    "id": "f028a9ef"
   },
   "outputs": [],
   "source": [
    "n_gram_counts=count_n_grams(data,1)\n",
    "n_plus1_gram_counts=count_n_grams(data,2)\n",
    "vocab_size=len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1c442b",
   "metadata": {
    "id": "4a1c442b"
   },
   "source": [
    "get_autocorrect utilise la fonction get_corrections pour obtenir les suggestions de correction pour un mot donné. Elle renvoie la correction la plus probable, c'est-à-dire le premier mot de la liste n_best retournée par get_corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f16c8",
   "metadata": {
    "id": "1f9f16c8"
   },
   "outputs": [],
   "source": [
    "def get_autocorrect(word):\n",
    "    # Get most probable correction\n",
    "    n_best = get_corrections(word, n_gram_counts,n_plus1_gram_counts,vocabulary, vocab_size ,n=2)\n",
    "    return n_best[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603a766",
   "metadata": {
    "id": "b603a766",
    "outputId": "91a4ad54-792b-4ce7-e7f0-8b9af04a56e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spending', 0.0003502626970227671, 4),\n",
       " ('swearing', 0.0003502626970227671, 4)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_l = process_data('input.txt')\n",
    "vocab = set(word_l)\n",
    "probs = get_probs(get_count(word_l))\n",
    "get_corrections(\"spealing\",n_gram_counts,n_plus1_gram_counts,vocabulary, vocab_size ,n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef4024",
   "metadata": {
    "id": "c7ef4024"
   },
   "source": [
    "# Algo 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fdde97",
   "metadata": {
    "id": "c4fdde97"
   },
   "source": [
    "un modele de langue qui modelise la distribution de s´equences de lettres au lieu des mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391bcbe",
   "metadata": {
    "id": "d391bcbe"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba11a92e",
   "metadata": {
    "id": "ba11a92e"
   },
   "source": [
    "extrait les mots de fichier après avoir effectué des traitements tels que la conversion en minuscules et la suppression des caractères non alphabétiques, et retourne une liste de ces mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0622c8",
   "metadata": {
    "id": "9d0622c8"
   },
   "outputs": [],
   "source": [
    "def process_data(text_file):\n",
    "    with open(text_file, 'r') as file:\n",
    "        text = file.read()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z ]', '', text)\n",
    "    word_l = text.split()\n",
    "    return word_l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4732d2cd",
   "metadata": {
    "id": "4732d2cd"
   },
   "source": [
    "get_trigrams(word) prend en entrée un mot et retourne une liste de tous les trigrammes présents dans ce mot. Un trigramme est une séquence de trois lettres consécutives dans un mot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322b66c6",
   "metadata": {
    "id": "322b66c6"
   },
   "outputs": [],
   "source": [
    "def get_trigrams(word):\n",
    "    trigrams = []\n",
    "    if len(word) < 3:\n",
    "        return trigrams\n",
    "    for i in range(len(word) - 2):\n",
    "        trigrams.append(word[i:i+3])\n",
    "    return trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737de7fb",
   "metadata": {
    "id": "737de7fb",
    "outputId": "3ccfd99a-e060-4bcb-8a00-aa0622201f97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wro', 'rog', 'ogn']"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test= get_trigrams(\"wrogn\")\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1841a68e",
   "metadata": {
    "id": "1841a68e"
   },
   "source": [
    "build_trigram_vocab(word_list) prend en entrée une liste de mots word_list et construit un vocabulaire de trigrammes. Le vocabulaire est représenté sous la forme d'un dictionnaire, où chaque clé est un trigramme et la valeur correspondante est un ensemble de mots qui contiennent ce trigramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5153e5e7",
   "metadata": {
    "id": "5153e5e7"
   },
   "outputs": [],
   "source": [
    "def build_trigram_vocab(word_list):\n",
    "    trigram_vocab = {}\n",
    "    for word in word_list:\n",
    "        trigrams = get_trigrams(word)\n",
    "        for trigram in trigrams:\n",
    "            if trigram in trigram_vocab:\n",
    "                trigram_vocab[trigram].add(word)\n",
    "            else:\n",
    "                trigram_vocab[trigram] = {word}\n",
    "    return trigram_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b11437",
   "metadata": {
    "id": "f8b11437",
    "outputId": "9b1efb4c-e31c-4e6a-851b-a2bb9c981781"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wro': {'wrogn'}, 'rog': {'wrogn'}, 'ogn': {'wrogn'}}"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=[\"wrogn\"]\n",
    "test=build_trigram_vocab(l)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2142a00",
   "metadata": {
    "id": "e2142a00"
   },
   "source": [
    "get_trigram_probs(word_l) prend en entrée une liste de mots word_l et calcule la fréquence des trigrammes dans ces mots. Elle utilise la classe Counter du module collections pour compter les occurrences de chaque trigramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad7d83",
   "metadata": {
    "id": "33ad7d83"
   },
   "outputs": [],
   "source": [
    "def get_trigram_probs(word_l):\n",
    "    trigram_count = Counter()\n",
    "    total_count = 0\n",
    "    for word in word_l:\n",
    "        trigrams = get_trigrams(word)\n",
    "        trigram_count.update(trigrams)\n",
    "        total_count += len(trigrams)\n",
    "    return trigram_count, total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16ffdd",
   "metadata": {
    "id": "7c16ffdd",
    "outputId": "da7153a2-578c-4498-9950-e83a7cca018e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'wro': 2, 'rog': 1, 'ogn': 1, 'ron': 1, 'ong': 1, 'ora': 1, 'ran': 1, 'ang': 1, 'nge': 1})\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "l=[\"wrogn\",\"wrong\",\"orange\"]\n",
    "trigram_count, total_count=get_trigram_probs(l)\n",
    "print(trigram_count)\n",
    "print(total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d5f149",
   "metadata": {
    "id": "c2d5f149"
   },
   "source": [
    "get_trigrams_count(word_list, trigram_vocab) prend en entrée une liste de mots word_list et un vocabulaire de trigrammes trigram_vocab. Elle compte le nombre d'occurrences de chaque trigramme dans les mots de la liste, en utilisant le vocabulaire de trigrammes comme référence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e0298",
   "metadata": {
    "id": "9d8e0298"
   },
   "outputs": [],
   "source": [
    "def get_trigrams_count(word_list, trigram_vocab):\n",
    "    trigram_count = {}\n",
    "\n",
    "    for word in word_list:\n",
    "        for i in range(len(word) - 2):\n",
    "            trigram = word[i:i+3]\n",
    "            if trigram in trigram_vocab:\n",
    "                if trigram in trigram_count:\n",
    "                    trigram_count[trigram] += 1\n",
    "                else:\n",
    "                    trigram_count[trigram] = 1\n",
    "\n",
    "    return trigram_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb68688",
   "metadata": {
    "id": "ecb68688",
    "outputId": "ec8f055f-20c3-4656-fcc7-9f25060efd17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wro': 2,\n",
       " 'rog': 1,\n",
       " 'ogn': 1,\n",
       " 'ron': 1,\n",
       " 'ong': 1,\n",
       " 'ora': 1,\n",
       " 'ran': 1,\n",
       " 'ang': 1,\n",
       " 'nge': 1}"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=[\"wrogn\",\"wrong\",\"orange\"]\n",
    "vocab=build_trigram_vocab(l)\n",
    "trigram_count=get_trigrams_count(l, vocab)\n",
    "trigram_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9cbe0a",
   "metadata": {
    "id": "ff9cbe0a"
   },
   "source": [
    "prend un mot (word), un dictionnaire de probabilités de trigrammes (trigram_probs), un dictionnaire de vocabulaire de trigrammes (trigram_vocab). Elle génère des suggestions de correction pour le mot donné en utilisant des trigrammes et la distance d'édition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1af78d",
   "metadata": {
    "id": "3b1af78d"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_corrections(word, trigram_probs, trigram_vocab, n=2, verbose=False):\n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "\n",
    "    if word in trigram_vocab:\n",
    "        return [(word, trigram_probs[word], 0)]\n",
    "\n",
    "    # Generate trigrams for the input word\n",
    "    input_trigrams = [word[i:i+3] for i in range(len(word) - 2)]\n",
    "\n",
    "    # Edit distance of 1\n",
    "    for trigram in input_trigrams:\n",
    "        suggestions.extend(trigram_vocab.get(trigram, []))\n",
    "\n",
    "    if suggestions:\n",
    "        n_best = [(suggestion, trigram_probs[suggestion], min_edit_distance(word, suggestion)[1]) for suggestion in suggestions]\n",
    "        if len(n_best) >= n:\n",
    "            return sorted(n_best, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "    # Edit distance of 2\n",
    "    for trigram in input_trigrams:\n",
    "        suggestions.extend(trigram_vocab.get(trigram, []))\n",
    "\n",
    "    if suggestions:\n",
    "        n_best = [(suggestion, trigram_probs[suggestion], min_edit_distance(word, suggestion)[1]) for suggestion in suggestions]\n",
    "        if len(n_best) >= n:\n",
    "            return sorted(n_best, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "    # Edit distance greater than 2\n",
    "    for i in range(3, n + 1):\n",
    "        for trigram in input_trigrams:\n",
    "            suggestions.extend(trigram_vocab.get(trigram, []))\n",
    "\n",
    "        if suggestions:\n",
    "            n_best = [(suggestion, trigram_probs[suggestion], min_edit_distance(word, suggestion)[1]) for suggestion in suggestions]\n",
    "            if len(n_best) >= n:\n",
    "                return sorted(n_best, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "    if not n_best:\n",
    "        return [(\"NO SUGGESTION\", 0.0, 0)]\n",
    "\n",
    "    return n_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ee44b2",
   "metadata": {
    "id": "05ee44b2"
   },
   "outputs": [],
   "source": [
    "word_l = process_data('input.txt')\n",
    "trigram_vocab = build_trigram_vocab(word_l)\n",
    "trigram_count,total_count = get_trigram_probs(word_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c0b2b",
   "metadata": {
    "id": "463c0b2b"
   },
   "outputs": [],
   "source": [
    "def get_autocorrect_trigram(word):\n",
    "    # Get most probable correction\n",
    "    n_best = get_corrections(word, trigram_count, trigram_vocab, n=2)\n",
    "    return n_best[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e9490",
   "metadata": {
    "id": "741e9490",
    "outputId": "7dc7fe77-9aa0-4d27-8fc8-4276d449ad86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: perdest\n",
      "Corrected word: perfect\n"
     ]
    }
   ],
   "source": [
    "word = \"perdest\"\n",
    "correction = get_autocorrect_trigram(word)\n",
    "print(\"Original word:\", word)\n",
    "print(\"Corrected word:\", correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f286316",
   "metadata": {
    "id": "6f286316"
   },
   "source": [
    "# Algo 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89a1efc",
   "metadata": {
    "id": "d89a1efc"
   },
   "source": [
    "noisy channel model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23141d9",
   "metadata": {
    "id": "e23141d9"
   },
   "outputs": [],
   "source": [
    "def get_probs(word_count):\n",
    "    total_count = sum(word_count.values())\n",
    "    word_probs = {}\n",
    "\n",
    "    for word, count in word_count.items():\n",
    "        word_probs[word] = count / total_count\n",
    "\n",
    "    return word_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3002c0ec",
   "metadata": {
    "id": "3002c0ec"
   },
   "outputs": [],
   "source": [
    "# UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: min_edit_distance\n",
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
    "    '''\n",
    "    Input:\n",
    "        source: a string corresponding to the string you are starting with\n",
    "        target: a string corresponding to the string you want to end with\n",
    "        ins_cost: an integer setting the insert cost\n",
    "        del_cost: an integer setting the delete cost\n",
    "        rep_cost: an integer setting the replace cost\n",
    "    Output:\n",
    "        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances\n",
    "        med: the minimum edit distance (med) required to convert the source string to the target\n",
    "    '''\n",
    "    # use deletion and insert cost as  1\n",
    "    m = len(source)\n",
    "    n = len(target)\n",
    "    #initialize cost matrix with zeros and dimensions (m+1,n+1)\n",
    "    D = np.zeros((m+1, n+1), dtype=int)\n",
    "\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "    # Fill in column 0, from row 1 to row m, both inclusive\n",
    "    for row in range(1,m+1): # Replace None with the proper range\n",
    "        D[row,0] = D[row-1,0] + del_cost\n",
    "\n",
    "    # Fill in row 0, for all columns from 1 to n, both inclusive\n",
    "    for col in range(1,n+1): # Replace None with the proper range\n",
    "        D[0,col] = D[0,col-1] + ins_cost\n",
    "\n",
    "    # Loop through row 1 to row m, both inclusive\n",
    "    for row in range(1,m+1):\n",
    "\n",
    "        # Loop through column 1 to column n, both inclusive\n",
    "        for col in range(1,n+1):\n",
    "\n",
    "            # Intialize r_cost to the 'replace' cost that is passed into this function\n",
    "            r_cost = rep_cost\n",
    "\n",
    "            # Check to see if source character at the previous row\n",
    "            # matches the target character at the previous column,\n",
    "            if source[row-1] == target[col-1]:\n",
    "                # Update the replacement cost to 0 if source and target are the same\n",
    "                r_cost = 0\n",
    "\n",
    "            # Update the cost at row, col based on previous entries in the cost matrix\n",
    "            # Refer to the equation calculate for D[i,j] (the minimum of three calculated costs)\n",
    "            D[row,col] = min([D[row-1,col]+del_cost, D[row,col-1]+ins_cost, D[row-1,col-1]+r_cost])\n",
    "\n",
    "    # Set the minimum edit distance with the cost found at row m, column n\n",
    "    med = D[m,n]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return D, med\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede3fc96",
   "metadata": {
    "id": "ede3fc96"
   },
   "source": [
    " prend un mot source (source_word), un vocabulaire de mots candidats (vocabulary), un dictionnaire de comptage de mots (word_count), ainsi que des coûts facultatifs pour les opérations d'insertion (ins_cost), de suppression (del_cost) et de remplacement (rep_cost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf7381",
   "metadata": {
    "id": "4ddf7381"
   },
   "outputs": [],
   "source": [
    "def noisy_channel_model(source_word, vocabulary,word_count, ins_cost=1, del_cost=1, rep_cost=2):\n",
    "    distances = {}\n",
    "\n",
    "    # Calculate edit distances between source sentence and candidate sentences\n",
    "    for vocab in vocabulary:\n",
    "        _, distance = min_edit_distance(source_word, vocab, ins_cost, del_cost, rep_cost)\n",
    "        distances[vocab] = distance\n",
    "\n",
    "    # Find the minimum edit distance\n",
    "    min_distance = min(distances.values())\n",
    "\n",
    "    # Find the candidate sentence(s) with the minimum edit distance\n",
    "    corrected_words = []\n",
    "    max_prob = 0\n",
    "    for word, distance in distances.items():\n",
    "        if distance == min_distance:\n",
    "            # Calculate the probability of each candidate word based on its frequency\n",
    "            word_probs = get_probs(word_count)\n",
    "\n",
    "            if word in word_probs:\n",
    "                prob = word_probs[word]\n",
    "\n",
    "                if prob > max_prob:\n",
    "                    corrected_words = [word]\n",
    "                    max_prob = prob\n",
    "                elif prob == max_prob:\n",
    "                    corrected_words.append(word)\n",
    "\n",
    "    return corrected_words[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a4c6d",
   "metadata": {
    "id": "a57a4c6d",
    "outputId": "a9262b32-ef75-41bc-e993-4e7990c5e7f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: spealing\n",
      "Corrected sentence: speaking\n"
     ]
    }
   ],
   "source": [
    "word_l = process_data('input.txt')\n",
    "vocab = set(word_l)\n",
    "\n",
    "\n",
    "source_word = \"spealing\"\n",
    "\n",
    "# Correct the misspelled word using the noisy channel model\n",
    "corrected_words = noisy_channel_model(source_word, vocab,get_count(word_l))\n",
    "\n",
    "print(f\"Original sentence: {source_word}\")\n",
    "print(f\"Corrected sentence: {corrected_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7d220",
   "metadata": {
    "id": "dab7d220"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
